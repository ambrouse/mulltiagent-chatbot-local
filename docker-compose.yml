services:
  # ------------------------------BACKEND-------------------------
  backend:
    build: ./backend
    container_name: fastapi_app
    ports:
      - "9000:8000"
    volumes:
      - ./backend:/app
    environment:
      # - MARKER_URL=http://marker-service:8000/convert-pdf/
      - LLM_API_URL=http://ollama:11434/v1/chat/completions
      - ENV=development
    
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload
    
  frontend:
    build: ./frontend  
    container_name: react_frontend
    ports:
      - "9002:9002"   
    volumes:
      - ./frontend:/app           
      - /app/node_modules         
    environment:
      - CHOKIDAR_USEPOLLING=true   
      - VITE_API_URL=http://localhost/api/v1   
    stdin_open: true              
    tty: true
    depends_on:
      - backend  

  nginx:
    image: nginx:alpine
    container_name: nginx
    ports:
      - "80:80"  
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - frontend
      - backend          


  # ---------------------------------------MODEL-------------------------------------
  
  
  # =========================
  # OLAMA MODEL (FULL LLM)
  # =========================
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    restart: always
    ports:
      - "8000:11434"
    volumes:
      - ./cache/ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    entrypoint: /bin/bash
    command: -c "ollama serve & sleep 5 && ollama pull qwen2.5:7b && wait"


  # =========================
  # MAKER PARSE PDF
  # =========================
  pdf-parser:
    build: ./marker
    image: marker-api-server
    container_name: marker_gpu
    ports:
      - "8001:8000"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - TORCH_DEVICE=cuda
      - IN_STREAMING_MODE=true  
    volumes:
      - ./cache/marker:/root/.cache
    restart: unless-stopped


  # =========================
  # PGE MODEL (EMBEDING AND RERANK)
  # =========================
  bge-service:
    build: ./embeding
    image: bge-api-server
    container_name: bge-service
    restart: unless-stopped
    ipc: host
    ports:
      - "8002:8000" 
    volumes:
      - ./cache/bge:/root/.cache/huggingface
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  

  # ----------------------------------------------------------DATABASE---------------------------------
  # =========================
  # postgress + ui
  # =========================
  postgres:
    image: postgres:15
    container_name: postgres
    environment:
      POSTGRES_USER: bao
      POSTGRES_PASSWORD: 3568
      POSTGRES_DB: multiangent_db
    ports:
      - "7000:5432"
    volumes:
      - ./cache/pgdata:/var/lib/postgresql/data

  db_ui:
    image: adminer
    container_name: chatbot_db_ui
    restart: always
    ports:
      - "7001:8080" 
    environment:
      - ADMINER_DEFAULT_SERVER=postgres 
    depends_on:
      - postgres 

  # =========================
  # Qdrant + UI
  # =========================
  qdrant:
    image: qdrant/qdrant:v1.3.0
    container_name: qdrant
    ports:
      - "7002:6333"      
    volumes:
      - ./cache/qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__WEB_ENABLE=true






networks:
  default:
    driver: bridge