import os
import torch
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Union, Dict, Any

# Import th∆∞ vi·ªán BAAI
from FlagEmbedding import BGEM3FlagModel, FlagReranker

app = FastAPI(title="Test BAAI Server SOTA")

# --- GLOBAL VARIABLES ---
embed_model = None
rerank_model = None

# --- CONFIG ---
if torch.cuda.is_available():
    DEVICE = "cuda"
    print(f"üî• PH√ÅT HI·ªÜN GPU: {torch.cuda.get_device_name(0)}")
else:
    DEVICE = "cpu"
    print("‚ö†Ô∏è C·∫¢NH B√ÅO: ƒêang ch·∫°y b·∫±ng CPU!")

EMBED_MODEL_ID = "BAAI/bge-m3"
RERANK_MODEL_ID = "BAAI/bge-reranker-v2-m3"

@app.on_event("startup")
async def startup_event():
    global embed_model, rerank_model
    
    # 1. Load Embedding
    # use_fp16=True: TƒÉng t·ªëc v√† gi·∫£m VRAM
    print(f"‚è≥ ƒêang t·∫£i Embedding: {EMBED_MODEL_ID} ...")
    embed_model = BGEM3FlagModel(EMBED_MODEL_ID, use_fp16=True, device=DEVICE)
    print("‚úÖ Embedding Model OK!")
    
    # 2. Load Reranker
    print(f"‚è≥ ƒêang t·∫£i Reranker: {RERANK_MODEL_ID} ...")
    rerank_model = FlagReranker(RERANK_MODEL_ID, use_fp16=True, device=DEVICE)
    print("‚úÖ Reranker Model OK!")

# --- REQUEST MODELS ---
class EmbedRequest(BaseModel):
    input: Union[str, List[str]]
    is_query: bool = False # <-- TH√äM C·ªú N√ÄY: ƒê·ªÉ ph√¢n bi·ªát c√¢u h·ªèi v√† t√†i li·ªáu

class RerankRequest(BaseModel):
    query: str
    documents: List[str]

# --- ENDPOINTS ---

@app.post("/embed")
async def create_embedding(req: EmbedRequest):
    """
    Tr·∫£ v·ªÅ c·∫£ Dense Vector (Ng·ªØ nghƒ©a) v√† Sparse Vector (T·ª´ kh√≥a)
    """
    sentences = [req.input] if isinstance(req.input, str) else req.input
    
    # X·ª¨ L√ù INSTRUCTION (QUAN TR·ªåNG CHO ƒê·ªò CH√çNH X√ÅC)
    # BGE-M3 ho·∫°t ƒë·ªông t·ªët nh·∫•t khi Query ƒë∆∞·ª£c th√™m ch·ªâ d·∫´n, c√≤n Doc th√¨ kh√¥ng
    if req.is_query:
        # Instruction chu·∫©n c·ªßa BGE cho retrieval
        # L∆∞u √Ω: BGE-M3 th√¥ng minh h∆°n b·∫£n c≈©, nh∆∞ng th√™m instruction v·∫´n gi√∫p ƒë·ªãnh h∆∞·ªõng t·ªët h∆°n
        # Tuy nhi√™n, th∆∞ vi·ªán FlagEmbedding th∆∞·ªùng t·ª± x·ª≠ l√Ω n·∫øu d√πng h√†m encode_queries
        # ·ªû ƒë√¢y ta d√πng encode chung n√™n c√≥ th·ªÉ gi·ªØ nguy√™n ho·∫∑c th√™m prefix n·∫øu c·∫ßn thi·∫øt.
        # V·ªõi BGE-M3, vi·ªác ph√¢n bi·ªát Query/Doc ch·ªß y·∫øu n·∫±m ·ªü c√°ch ta d√πng vector sau n√†y.
        pass 

    # encode tr·∫£ v·ªÅ dictionary ch·ª©a: dense_vecs, sparse_vecs, colbert_vecs
    output = embed_model.encode(
        sentences, 
        batch_size=12, 
        max_length=8192,
        return_dense=True,   # L·∫•y vector ng·ªØ nghƒ©a
        return_sparse=True,  # <--- L·∫§Y TH√äM C√ÅI N√ÄY (Lexical Weights)
        return_colbert_vecs=False # T·∫Øt c√°i n√†y ƒëi cho nh·∫π (tr·ª´ khi b·∫°n d√πng ColBERT)
    )
    
    # Chu·∫©n b·ªã k·∫øt qu·∫£ tr·∫£ v·ªÅ
    dense_data = output['dense_vecs'].tolist()
    
    # Sparse vector tr·∫£ v·ªÅ d·∫°ng dictionary {token_id: weight}, ta c·∫ßn x·ª≠ l√Ω ch√∫t ƒë·ªÉ tr·∫£ JSON
    # output['lexical_weights'] l√† list c√°c dict
    sparse_data = output['lexical_weights'] 

    return {
        "object": "list",
        "data": [
            {
                "index": i,
                "embedding": dense_data[i], # Vector ng·ªØ nghƒ©a (D√πng cho vector search)
                "sparse_embedding": sparse_data[i] # Vector t·ª´ kh√≥a (D√πng cho keyword boosting)
            } 
            for i in range(len(dense_data))
        ]
    }

@app.post("/rerank")
async def rerank_docs(req: RerankRequest):
    if not req.documents: return {"results": []}
    
    # Reranker BGE-M3 t·ª± ƒë·ªông x·ª≠ l√Ω ng·ªØ nghƒ©a v√† t·ª´ kh√≥a b√™n trong n√≥
    pairs = [[req.query, doc] for doc in req.documents]
    
    scores = rerank_model.compute_score(pairs, batch_size=12, max_length=2048) # TƒÉng max_length n·∫øu t√†i li·ªáu d√†i
    
    if isinstance(scores, float): scores = [scores]
    
    results = [{"index": i, "score": s, "text": req.documents[i]} for i, s in enumerate(scores)]
    results.sort(key=lambda x: x["score"], reverse=True)
    return {"results": results}